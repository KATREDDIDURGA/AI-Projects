# ðŸ§  LLM-EvalFlow

**LLM-EvalFlow** is a real-world, minimal LLM evaluation system I built to simulate the kinds of workflows used in AI infrastructure and deployment teams â€” especially for human-in-the-loop (HITL), QA, and GenAI feedback loops.

It uses the **Groq API**, open-source Python tools, and a clean **Streamlit dashboard** to test, evaluate, and review LLM outputs across prompts.

---

## ðŸ”§ Tech Stack

- **Python** (modular scripts)
- **Groq API** (LLM response generation)
- **Streamlit** (UI dashboard)
- **FastAPI** (optional API layer)
- **Pandas & JSON** (data handling)

---

## ðŸŽ¯ Features

- âœ… Load prompts from CSV  
- âœ… Send prompts to LLM using Groq API  
- âœ… Evaluate responses using custom rules  
- âœ… Log and view feedback (score + notes)  
- âœ… Test new prompts interactively via UI  
- âœ… Simple and extensible design  

---


<img src="https://github.com/KATREDDIDURGA/AI-Projects/a
